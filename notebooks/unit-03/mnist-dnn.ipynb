{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST DNN Project\n",
    "\n",
    "##### How to work through this project:\n",
    "- Go cell by cell and finish the marked #TODO's\n",
    "- You don't need to touch the code marked between the `#---------#`. Those are puzzle pieces that your code will fit into!\n",
    "    - However, I **STRONGLY** encourage you to understand every single line between those blocks. They are essential!\n",
    "    - It is crucial that your variable names are what we expect them to be, or the puzzle pieces won't fit.\n",
    "- Tutorials/helpful information will be placed in the `.md` cells above the \"work\" cells. Consult them if you are stuck.\n",
    "- If you REALLY cannot find the correct code to make the cell run, consult the `[proj]-ans.ipynb`.\n",
    "- The final product (what we expect to see if you run all the cells consecutively) will be placed in the `answers/` directory.\n",
    "    - Chances are your output won't be the exact same (stochasticity!) but it should be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get used to these imports!\n",
    "#----------------------------------------------------------------#\n",
    "#To install: pip install numpy\n",
    "import numpy as np \n",
    "#To install: pip install matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "#To install: pip install sklearn\n",
    "import sklearn \n",
    "#To install: pip install torchvision\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "#To install: pip install torch (not GPU compatible)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the train/test data from MNIST. Try and find their shapes\n",
    "#----------------------------------------------------------------#\n",
    "train_data = tv.datasets.MNIST('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = tv.datasets.MNIST('./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "#----------------------------------------------------------------#\n",
    "## TODO: Find and print the shapes of train_data and test_data\n",
    "# Print the shape of data and the shape of labels\n",
    "\"\"\"\n",
    "Tips:\n",
    "- To get just the data from (train/test)_data, you will need to do (train/test)_data.data\n",
    "- To get just the labels from (train/test)_data, you will need to do (train/test)_data.targets\n",
    "- Then use .numpy() to convert the data into a numpy array, which you can then call .shape on\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block is short, but ***incredibly important***. It shows how to set up a **Dataloader** which is needed to pass data through a neural network. Try to get familiar with the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataloaders from the MNIST dataset\n",
    "batch_size = 60\n",
    "#----------------------------------------------------------------#\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False) \n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the data! This will be different than last time since we are workign with dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes num_to_viz digits and labels with plt.matshow and plt.show. Notice how reshape is used to get the data into proper format for visualization.\n",
    "# Note the use of reshape!\n",
    "#----------------------------------------------------------------#\n",
    "num_of_digits_to_viz = 3\n",
    "for i in range(num_of_digits_to_viz):\n",
    "    to_reshape = train_data.data.numpy()[i]\n",
    "    plt.matshow(to_reshape.reshape(28, 28))\n",
    "    plt.show()\n",
    "    print(f\"Associated Label: {train_data.targets.numpy()[i]}\")\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its the time you've all been waiting for... Making your own neural network! Below an example network is defined **but it won't work with MNIST**. Try and use this syntactical example, along with google, to construct a DNN that works with the MNIST dataset! **This is most of the work of the project. Please try and do it!**\n",
    "\n",
    "If you get stuck, take a look at how this tutorial did it:\n",
    "- [MNIST DNN Tutorial](https://analyticsindiamag.com/guide-to-feed-forward-network-using-pytorch-with-mnist-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a neural network that can classify MNIST\n",
    "# Provided below is a syntactical example of a DNN, study it and try and make one that will fit MNIST\n",
    "\n",
    "\"\"\"\n",
    "class DNN_EXAMPLE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN_EXAMPLE, self).__init__()\n",
    "\n",
    "        # Structure\n",
    "        self.input_layer_size = 104\n",
    "        self.hidden_layer_A_size = 51\n",
    "        self.output_layer_size = 17\n",
    "\n",
    "        # Layers\n",
    "        self.l1 = nn.Linear(self.input_layer_size, self.hidden_layer_A_size)\n",
    "        self.l2 = nn.Linear(self.hidden_layer_A_size, self.output_layer_size)\n",
    "\n",
    "        # Activations\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.l1(input)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.l2(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "\"\"\"\n",
    "\n",
    "## TODO: Create the following required NN class that can work with MNIST data, and then instantiate a model\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "###! MAKE SURE YOUR NEURAL NETWORK HAS AT LEAST 3 HIDDEN LAYERS AND DOES NOT HAVE HARDCODED LAYER SIZE VALUES!###\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#\n",
    "\n",
    "# MNIST_DNN: The name of your class\n",
    "# model: An instance of MNIST_DNN\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Think about what input and output sizes you want\n",
    "- Hidden layers can be most anything, just make sure to reduce gradually\n",
    "- Remind yourself what activations are and why they are useful\n",
    "- Make sure to name your class \"MNIST_DNN\"\n",
    "- Instead of hardcoding the numbers in for the layer sizes, make them passable parameters\n",
    "- Make sure to actually make your model using model = MNIST_DNN(...) as the last line\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome job! Feel free to check `mnist-dnn-ans.ipynb` to make sure you're on the right track. Next we will define our loss function and our method of optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to mess with the code in here once you have finished the project to see what effect it will have.\n",
    "# For now, though, simply read and accept the syntax as-is\n",
    "#----------------------------------------------------------------#\n",
    "loss_func = nn.CrossEntropyLoss() # Mean Squared Error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Adaptive Optimizer\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make the training loop. Take a look at the \"Creating our Training Loop\" subsection of the tutorial below and transfer the code over. If it doesn't work, remmeber you always have the answer notebook to get you unstuck. **Remove all calls to \"`.to(device)`\"**\n",
    "\n",
    "- [MNIST DNN Tutorial](https://analyticsindiamag.com/guide-to-feed-forward-network-using-pytorch-with-mnist-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "## TODO: Implement the training loop as shown in the tutorial, but remove all calls to .to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we move onto the testing loop! This isn't incredibly difficult code so it's just provided below, but take a look at how it's structured and what it is doing. \n",
    "\n",
    "Then run all your code and see what your **final accuracy** is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the use of torch.no_grad() and torch.max(). Be sure you know what they are doing\n",
    "#----------------------------------------------------------------#\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item() \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %') \n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions:\n",
    "- Mess with model hyperparameters, training epochs, data size, optimizers, loss functions, anything!\n",
    "- Get accuracy above 97% consistently"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on completing the project! Check your results with the notebook in the `answers` directory and then send your final accuracy to your club/channel/mentor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3204eab9621eb34088b9e71fcdb754ce79a12fd6a4cd73f4898b86bb3d12718"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
