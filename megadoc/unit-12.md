---
title: Unit 12
parent: Megadoc
---

# Unit 12: Human Characteristics of the Brain

Several traits of humans appear to be unique to our species and may be essential in designing an intelligence similar to our own. This chapter is dedicated to the study of these phenomena of our brains. We begin by trying to peel back the layers of humans’ strong social tendencies:

**Task 1:** Complete the following two lectures and synthesis questions.

[20. Theory of Mind & Mentalizing](https://www.youtube.com/watch?v=pfZY5aDJazA&list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP&index=15)


### `Synthesis Questions:`

* `Describe the false belief paradigm and what function it serves.`
* `What part of the brain is/are specifically involved in thinking about others' thoughts? Where is it located? What are several close by modules and their functions?`
* `Google the terms TMS, EEG, and DBS. What are several differences and similarities between these cognitive science methods?`

[Social learning in independent multi-agent reinfor… \| Kamal N’dousse \| OpenAI Scholars Demo Day 2020](https://www.youtube.com/watch?v=Qy9J5519s68)


### `Synthesis Questions:`

* `What systems might the methods of RL used (DQN, PPO, etc…) lack that make learning theory of mind or social learning more difficult for AI than humans?`
    * `Research several more areas of the brain or human skills implicated in social learning. Does RL or any area of ML research incorporate these concepts?`

**Task 2:** Refresh yourself on the use of attention mechanisms in transformers in machine learning with this quick video, then dig into the next lecture and corresponding questions.

[Attention Mechanism In a Nutshell](https://www.youtube.com/watch?v=oMeIDqRguLY)
A very comprehensive and visually helpful intuition for what the attention mechanism actually does:  
Optional [3Blue1Brown: Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc)
If you want to play with the math behind LLMs, this is pretty cool:  
Optional [Self Attention Colab Notebook](https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb)

For a deeper dive, feel free to take a look at the Language Modeling chapter of this document. Now we will dive into the capabilities of the brain and its own beautiful, endogenous attention mechanisms!

[24. Attention and Awareness](https://www.youtube.com/watch?v=B4a0WdGp52g&list=PLUl4u3cNGP60IKRN_pFptIBxeiMc0MCJP&index=17)


### `Synthesis Questions:`

* `Describe the brain's ability to multitask? What are some scenarios where parallel processing is feasible and some where it is not?`
* `Describe covert and overt attention.`
* `Research some machine learning attention mechanisms. Compare and contrast the mechanism and the capabilities of one with what you learned about our biological attention mechanism.`
* `Describe "priming the visual cortex" why it might be useful.`
* <code>"You have 10x as many connections going down from cortex, down to the LGN ([lateral geniculate nucleus](https://en.wikipedia.org/wiki/Lateral_geniculate_nucleus))… than going forward. One of the things you're doing is setting up selective filters so that only the stuff you want to process makes it to higher stages." Compare this fact to deep learning algorithms.</code>
* <code>Describe the role of the Fronto-Parietal Attention Network"</code>

# Project Spec
Consider, for a moment, the ways in which there are or are not parallels between our current mechanisms of machine learning and the various modules and functions of the brain that we have learned about. In your opinion, are some crucial for intelligence? Which, if any, have we managed to emulate with algorithms?

Now consider attention. The brain seems to block out unwanted information from ever being processed (about minute 8:00, Attention and Awareness). Are there ML algorithms that do this? Should they? What should the function of attention be? Does it depend on the context or is it a fixed algorithm for all contexts?

Write some thoughts (200+ words) on these questions. Then, finally, find one article on [Grey Matters](https://greymattersjournal.org/) that interests you and consider how it relates to the above questions. Provide the link and some thoughts :\)