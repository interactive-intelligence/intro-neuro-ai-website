
<!DOCTYPE html>
<html lang="en-US">
<head>
 <meta charset="UTF-8">
 <meta http-equiv="X-UA-Compatible" content="IE=Edge">
<link rel="stylesheet" href="/pr-preview/pr-38/assets/css/just-the-docs-default.css">
    <script src="/pr-preview/pr-38/assets/js/vendor/lunr.min.js"></script>
  <script src="/pr-preview/pr-38/assets/js/just-the-docs.js"></script>
 <meta name="viewport" content="width=device-width, initial-scale=1">
<title>Unit 04 | I2 Intro Neuro/AI</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Unit 04" />
<meta name="author" content="Interactive Intelligence" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<link rel="canonical" href="https://course.uw-i2.org/pr-preview/pr-38/megadoc/unit-04/" />
<meta property="og:url" content="https://course.uw-i2.org/pr-preview/pr-38/megadoc/unit-04/" />
<meta property="og:site_name" content="I2 Intro Neuro/AI" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Unit 04" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Unit 04","url":"https://course.uw-i2.org/pr-preview/pr-38/megadoc/unit-04/"}</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true },
          { left: '$', right: '$', display: false },
          { left: '\\(', right: '\\)', display: false },
          { left: '\\[', right: '\\]', display: true }
        ],
        // • rendering keys, e.g.:
        throwOnError: false
      });
    });
  </script>
<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
 <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
   <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
 <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
   <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
 <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
   <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
 <title id="svg-external-link-title">(external link)</title>
 <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
 <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
   <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
 <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>
<symbol id="svg-copy" viewBox="0 0 16 16">
 <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
   <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
 <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
   <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>
</svg>
 <div class="side-bar">
 <div class="site-header">
    <a href="/pr-preview/pr-38/" class="site-title lh-tight">
  I2 Intro Neuro/AI
</a>
    <a href="#" id="menu-button" class="site-button">
      <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
    </a>
 </div>
 <nav aria-label="Main" id="site-nav" class="site-nav">
     <ul class="nav-list"><li class="nav-list-item"><a href="/pr-preview/pr-38/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item"><a href="/pr-preview/pr-38/staff/" class="nav-list-link">Course Staff</a><li class="nav-list-item"><a href="/pr-preview/pr-38/graduates/" class="nav-list-link">Graduates</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Megadoc category">
            <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg>
          </a><a href="/pr-preview/pr-38/megadoc/" class="nav-list-link">Megadoc</a><ul class="nav-list"><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-01/" class="nav-list-link">Unit 01</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-02/" class="nav-list-link">Unit 02</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-03/" class="nav-list-link">Unit 03</a><li class="nav-list-item  active"><a href="/pr-preview/pr-38/megadoc/unit-04/" class="nav-list-link active">Unit 04</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-05/" class="nav-list-link">Unit 05</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-06/" class="nav-list-link">Unit 06</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-07/" class="nav-list-link">Unit 07</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-08/" class="nav-list-link">Unit 08</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-09/" class="nav-list-link">Unit 09</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-10/" class="nav-list-link">Unit 10</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-11/" class="nav-list-link">Unit 11</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-12/" class="nav-list-link">Unit 12</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-13/" class="nav-list-link">Unit 13</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-A/" class="nav-list-link">Unit A</a><li class="nav-list-item "><a href="/pr-preview/pr-38/megadoc/unit-B/" class="nav-list-link">Unit B</a></ul><li class="nav-list-item"><a href="/pr-preview/pr-38/schedule/" class="nav-list-link">Schedule</a></ul>
 </nav>
   <footer class="site-footer">
      This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
   </footer>
</div>
 <div class="main" id="top">
   <div id="main-header" class="main-header">
<div class="search">
 <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
 </div>
 <div id="search-results" class="search-results"></div>
</div>
   <nav aria-label="Auxiliary" class="aux-nav">
 <ul class="aux-nav-list">
     <li class="aux-nav-list-item">
        <a href="https://interactive-intelligence.github.io" class="site-button"
        >
          UW Interactive Intelligence Website
        </a>
 </ul>
</nav>
</div>
   <div id="main-content-wrap" class="main-content-wrap">
   <nav aria-label="Breadcrumb" class="breadcrumb-nav">
     <ol class="breadcrumb-nav-list">
         <li class="breadcrumb-nav-list-item"><a href="/pr-preview/pr-38/megadoc/">Megadoc</a>
       <li class="breadcrumb-nav-list-item"><span>Unit 04</span>
     </ol>
   </nav>
     <div id="main-content" class="main-content" role="main">
         <h1 id="unit-4-computer-vision">
    <a href="#unit-4-computer-vision" class="anchor-heading" aria-labelledby="unit-4-computer-vision"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 4: Computer Vision
</h1>
<p>Hello and welcome to the <em>Basics</em> section of the I2 megadoc!</p>
<p><strong>Task 1:</strong> Read either the <ins>literacy article</ins> “Back to Basics” or the <ins>technical article</ins> linked below to get an intuitive understanding of computer vision. <span style="color:red"><strong>This is required.</strong></span></p>
<h4 id="unit-04-technical-article">
    <a href="#unit-04-technical-article" class="anchor-heading" aria-labelledby="unit-04-technical-article"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="/pr-preview/pr-38/megadoc/unit-04-tech-article/">Unit 04 Technical Article</a>
</h4>
<p> </p>
<p><strong>Task 2:</strong> Go through the following videos/articles and answer the provided synthesis questions. Submit your answers to your intro course TA. 
<a href="https://course.uw-i2.org/megadoc/unit-04/#unit-4-synthesis-questions">Link to this task</a></p>
<p><strong>Task 3:</strong> Complete either the technical project or the non-technical project. Submit your work to the intro course TA.
<a href="https://course.uw-i2.org/megadoc/unit-04/#unit-4-project-specs">Link to this task</a></p>
<h2 id="back-to-basics-computer-vision">
    <a href="#back-to-basics-computer-vision" class="anchor-heading" aria-labelledby="back-to-basics-computer-vision"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Back to Basics: Computer Vision
</h2>
<p>In Week 2, we talked about how deep neural networks work on a very general level. Today, we’re going to talk about a new type of neural network, called a Convolutional Neural Network (CNN). These are neural networks that are specifically designed to process images. While neural networks resemble the way the human brain works, CNNs resemble the way the human vision system works. Often, a CNN is a subsection of a larger neural network: it’s like a group of layers that exists to process images really efficiently.</p>
<p>Let’s think back to our discussion of neural networks in unit 2. Recall that we used the MNIST dataset, which contains images of handwritten numbers (here’s a picture to remind you, with <a href="https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe">credit</a>):</p>
<div style="text-align:center">
    <img src="../assets/unit2/literacy_images/labeled_mnist.webp" alt="Labeled images from the MNIST database" width="500" />
</div>
<p>Near the end of the article, we mentioned that as we kept training our neural network, the computer would begin to pick out patterns associated with each number (like ones usually being a straight vertical line, or zeroes being a circle). But how exactly does the computer figure that out? How does the neural network “realize” what a one, a zero, or any other number looks like?</p>
<p>This is where the <strong>convolutional layer</strong> comes in. A CNN uses its convolutional layers to identify patterns and utilize them to analyze images. A convolutional layer would pick out the features that distinguish one number from the others.</p>
<p>Recall the structure of a general neural network.</p>
<div style="text-align:center">
    <img src="../assets/unit2/literacy_images/neural_network.png" alt="Diagram of a neural network" width="500" />
</div>
<p>Notice that every single node in the input layer is connected to every single node in the hidden layer. Remember, every node in the input layer corresponds to a pixel in the image. Our pictures of handwritten numbers contain 784 pixels (28 x 28), which means there are 784 nodes in the input layer. This means we have a lot of connections between the input layer and the first hidden layer, and the first hidden layer and the second hidden layer, and so on.</p>
<p>The reason the neural network is set up like this is because it assumes that information about every pixel influences every other pixel. But that’s not necessarily true! For example, in each image in the MNIST dataset, the dark pixels tend to be close to other dark pixels, and the light pixels tend to be close to other light pixels.</p>
<p>Also, the patterns associated with a given number are not exactly the same every time. Take a look at the image below (<a href="https://towardsdatascience.com/part-5-training-the-network-to-read-handwritten-digits-c2288f1a2de3">credit</a>) and notice how there are slight differences in the way the numbers are written (e.g. the number 7 vs. 7 with a line through it). We need the computer to recognize that these are just variations on the exact same number.</p>
<div style="text-align:center"> 
    <img src="../assets/unit4/literacy_images/various_numbers.png" alt="Various numbers in the MNIST dataset" width="500" />
</div>
<p>CNNs account for both of these things, reducing the number of input nodes and allowing for variations in the pixels when identifying features in an image.</p>
<p>Now we’re going to get into how CNNs actually work, using a very simple image of a handwritten number.</p>
<p>Take a look at the image below, which shows the number 8. We’ve assigned a numerical value to each pixel, like was discussed in Unit 2.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/cnn_sample_8.png" alt="Sample image showing the number 8, with pixels labeled and numbered" width="300" />
</div>
<p>The first thing a CNN does is use filters to identify the locations of features in the image. In our example, the most important feature of an 8 is an ellipse, because an 8 is made of two ellipses stacked on top of each other. So let’s use a filter to identify any ellipses in the image. At first, our filter just contains random numbers—but through backpropagation, it starts to resemble the shape of the feature we’re looking for (an ellipse). Below is an example of what the correct filter would look like.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/cnn_filter.png" alt="Sample filter for above image" width="300" />
</div>
<p>Next, we put the filter over the image and compute the <strong>dot product</strong> of the filter and the portion of the image that it overlaps. This involves multiplying the value in the filter by the value of the pixel underneath, and adding up all the products.</p>
<p>Take a look at the example below. Notice that the feature in the filter (the ellipse) doesn’t perfectly match up with the pixels underneath it, so our dot product ends up being 4.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/conv1.png" alt="Diagram of first convolution" width="500" />
</div>
<p>Then, we shift our filter over by one and do the same thing. The distance by which we move our filter is called the <strong>stride</strong>. Notice that in this next example, the feature in the filter perfectly matches with the pixels underneath, so our dot product is much larger (8).</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/conv2.png" alt="Diagram of second convolution" width="500" />
</div>
<p>We’ll continue shifting the filter until we’ve covered the entire image (there will probably be a lot of overlap). We keep track of all these dot products in a <strong>feature map</strong>. The feature map is useful because it tells us the general locations of the feature we’re tracking. Our feature map is a 3 by 3 matrix because we apply the filter nine times overall (you can try shifting the filter and calculating the dot products yourself!).</p>
<p>Remember, we said that when the filter perfectly matched the pixels underneath (i.e., there was an ellipse in the image), the dot product was 8. The feature map has two 8s, which tells us that there are two ellipses in the image: in the top middle of the image and in the bottom middle of the image. This fits with what we know an 8 should look like: two ellipses stacked on top of each other!</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/feature_map.png" alt="Diagram of feature map" width="500" />
</div>
<p>The entire process we just did, of converting an image into a feature map, is called a <strong>convolution</strong>.</p>
<p>The image we’re using, again, is really simple: it only has one distinguishing feature, which is its ellipses. If our image has multiple features (e.g. multiple lines, angles, edges, curves, etc.), we would use a different feature map for each one, and do convolutions for each feature all at the same time!</p>
<p>Next, we’re going to simplify this information a little bit using a strategy called <strong>pooling</strong>. Pooling means to reduce our feature map into an even smaller matrix that contains the most important information from each “region” of the image. Our image is a little too simple to pool any further, but below is an example of how that would work (<a href="https://paperswithcode.com/method/max-pooling">credit</a>). Here, the entire top left corner is simplified to just the largest value, and so on for the other regions. This gives us a simplified understanding of the feature map overall.</p>
<p>The example below uses “max pooling,” which means it takes the largest value from each region to represent the region as a whole. There are other types of pooling as well, such as “average pooling,” which takes the average of the region to represent the overall region.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/pooling.png" alt="Example of max pooling" width="500" />
</div>
<p>We repeat the convolution-pooling cycle until our feature map (or maps, if we have multiple features) is sufficiently small. Finally, we plug our resulting feature maps into a <strong>fully connected layer</strong>, which is like a regular old neural network. Before, all our features were analyzed independently in different convolutions. Here, we’re putting everything together and using all our collected information to classify our image. We do this by taking all of our small, pooled feature maps, flattening them into a column vector, and treating this column vector as the <strong>input layer</strong> to a standard neural network, which then predicts what the final image is. Take a look at the image below for an example (<a href="https://slds-lmu.github.io/seminar_nlp_ss20/convolutional-neural-networks-and-their-applications-in-nlp.html">credit</a>).</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/fully_connected.png" alt="Example of fully connected layer" width="500" />
</div>
<p>This was a lot of information, so please reach out to a TA if you’re having trouble with these concepts or with the homework! Since this article was very abstract and conceptual, the homework is going to be much more technically focused.</p>
<h2 id="unit-4-synthesis-questions">
    <a href="#unit-4-synthesis-questions" class="anchor-heading" aria-labelledby="unit-4-synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 4 Synthesis Questions
</h2>
<h3 id="video-1-how-convolutional-neural-networks-work-12-min">
    <a href="#video-1-how-convolutional-neural-networks-work-12-min" class="anchor-heading" aria-labelledby="video-1-how-convolutional-neural-networks-work-12-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 1:</strong> <a href="https://youtu.be/FmpDIaiMIeA?t=840">How Convolutional Neural Networks work</a> <strong>(12 min)</strong>
</h3>
<p><strong>Note:</strong> Watch from 13:54 onward to answer the questions below. Before that is mainly review from this article (but you may find it useful to skim through, as it covers helpful math concepts!).</p>
<h3 id="synthesis-questions">
    <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><em>How is backpropagation used in CNNs, and how does it differ from backpropagation in standard neural networks?</em>
 <li><em>What outcomes can a designer achieve from adjusting the hyperparameters or architecture of a CNN?</em>
 <li><em>Can you think of an example of when we can use a CNN on non-image data?</em>
</ul>
<h3 id="video-2-but-what-is-a-convolution-14-min">
    <a href="#video-2-but-what-is-a-convolution-14-min" class="anchor-heading" aria-labelledby="video-2-but-what-is-a-convolution-14-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 2:</strong> <a href="https://www.youtube.com/watch?v=KuXjwB4LzSA">But what is a convolution?</a> <strong>(14 min)</strong>
</h3>
<p><strong>Note:</strong> Watch up to 13:42 in this video; the rest is beyond the scope of this course.</p>
<h3 id="synthesis-questions-1">
    <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><em>What is the name for the smaller grid that convolves over a larger image?</em>
   <ul>
     <li><em>Hint: Starts with a “k”</em>
   </ul>
 <li><em>What are some examples of what you can do to images if you convolve them with special matrices?</em>
 <li><em>How does Gaussian blur “work”?</em>
 <li><em>What is the name for the actual operation that occurs when the smaller grid is overlaid on the larger one?</em>
   <ul>
     <li><em>When each element of the corresponding pixels are multiplied then summed.</em>
   </ul>
 <li><em>Give an example of a 3x3 matrix that would not do anything to the image it convolves over. Why does it not impact the image?</em>
   <ul>
     <li><em>This is also known as the “do-nothing” matrix</em>
   </ul>
</ul>
<h2 id="unit-4-project-specs">
    <a href="#unit-4-project-specs" class="anchor-heading" aria-labelledby="unit-4-project-specs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unit 4 Project Specs
</h2>
<h3 id="non-technical-project-spec">
    <a href="#non-technical-project-spec" class="anchor-heading" aria-labelledby="non-technical-project-spec"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Non-Technical Project Spec:</strong>
</h3>
<p>The non-technical project for this unit will involve some writing! <strong>Choose 3</strong> of the prompts below and write <strong>at least 200</strong> (<em>meaningful!</em>) words on each one! We will not be strictly grading you on correctness or anything like that. This is an opportunity to deeply engage with the material you have just learned about, and creatively connect it to neuroscience!</p>
<ul>
 <li>How are CNNs inspired by the human visual system?
 <li>What are some similarities and differences between CNNs and the human visual system?
 <li>How is the pooling layer in CNNs related to the brain’s visual processing?
 <li>What ways does the convolutional layer in CNNs resemble the receptive field in the visual system?
 <li>Reflecting on you have learned from this unit, what is one thing you found to be most interesting?
 <li>What is one concept from this unit that you would like to learn more about and why?
</ul>
<p>Be sure to submit your work through google drive using the submission form!
We would prefer that you upload it to your own Drive first, then use the submission form dropbox to connect that file to your submission!</p>
<h3 id="technical-project-spec">
    <a href="#technical-project-spec" class="anchor-heading" aria-labelledby="technical-project-spec"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Technical Project Spec:</strong>
</h3>
<p>The project for this “<em>Computer Vision</em>” section will be following the tutorial/Jupyter Notebook below. Please ask questions in the discord as you work through this project. Be sure to discuss with others in your group!</p>
<p>A few general helpful tips (if applicable):</p>
<ul>
 <li>Be sure to appropriately make a copy of the Colab template before starting to save your progress!
 <li>Renaming your copy to something that contains your name is a good idea, it will make it easier for us to review your submissions.
 <li>Leave comments to cement your understanding. Link syntax to ideas.
 <li><strong>Read up on what <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10</a> is.</strong>
</ul>
<p>Now, follow the instructions on this Jupyter notebook to implement some of the things we talked about. There is an “answers” link at the bottom of the notebook that you can use if stuck. You will need to download the ‘.ipynb’ found in that directory and open it either locally or in a new colab project yourself. Ask around if you are unable to get it working!</p>
<p><strong>Colab Link:</strong> <a href="https://colab.research.google.com/drive/1kvPA3EyFvmG4pknRY2uY9ZYs_cIBL2WF?usp=sharing">Unit 4 Notebook</a> <strong>(1 hr)</strong></p>
<p>When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas.</p>
<p>Remember that this is all for your learning, so do your best and don’t stress!</p>
<p>Congratulations! You now understand the basics of Convolutional Neural networks!</p>
     </div>
   </div>
<div class="search-overlay"></div>
 </div>
  
