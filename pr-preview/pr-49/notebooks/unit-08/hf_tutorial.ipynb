{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# What is this notebook?\n",
        "Hugging Face is a popular library and resouce for training and using AI models. While it has many valuable resources, it can be extremely difficult to use. This notebook aims to serve as an introduction to Hugging Face and all the tools it provides."
      ],
      "metadata": {
        "id": "35Q_4O9HM-Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "pysjik7mNCQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might want a Hugging Face account. If you don't have one already, sign up [here](https://huggingface.co/join)!"
      ],
      "metadata": {
        "id": "j8sudHdocbh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the necessary libraries!\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "H-Vt2eEnVbq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of HuggingFace"
      ],
      "metadata": {
        "id": "f3bE1FYBNFot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(IMO) Hugging Face serves two tasks: storage of AI resouces (models, tokenizers, datasets) and a library of tools for training/using AI models. These resources take the form of a Github-like repository service (the HF Hub) in addition to libraries.\n",
        "\n",
        "Hugging Face's prominent library is [transformers](https://github.com/huggingface/transformers/), a library containing powerful foundation (pretrained) models and tools to use them. Hugging Face also has a [tokenizers](https://github.com/huggingface/tokenizers) library for tokenizers, a [datasets](https://github.com/huggingface/datasets) library for datasets, and a [diffusers](https://github.com/huggingface/diffusers) library for, you guessed it, diffusion models. (They have a lot of stuff. Too much stuff IMO. It is a bit overwhelming.)\n",
        "\n",
        "Hugging Face has the [Hub](https://huggingface.co/docs/hub/index), a Github-like service for storing models, datasets, and more. You can use this to store trained models or datasets or access others pre-trained models!"
      ],
      "metadata": {
        "id": "i1Hl12swQm35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The good of Hugging Face:\n",
        "\n",
        "- Easy to store and manage models and datasets.\n",
        "- Has many important pre-trained models and popular datasets.\n",
        "- Has some really powerful tools.\n",
        "\n",
        "The bad of Hugging Face:\n",
        "- The documentation can be terrible.\n",
        "- Incredibly confusing - there's too much going on.\n",
        "- Poor abstractions - too often I have to *give in to the Great Hugging Face* and just trust that it works.\n",
        "- Little consistency in classes, data structures, etc."
      ],
      "metadata": {
        "id": "aphmE_rl2yZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's use a pretrained model!"
      ],
      "metadata": {
        "id": "_DAnDXXcOCGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, enough talking. Let's get to something fun!\n",
        "\n",
        "The first way to use a model is with a [`pipeline`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines). A `pipeline` is a crazy abstraction that reduces a bunch of \"scary AI stuff\" into a simple object for inference. We simply need to give the `pipeline` a task or model at instantiation and it is ready for inference. Take a look:"
      ],
      "metadata": {
        "id": "RMv9z22oT92R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\") # text-classification is the task\n",
        "pipe(['Wow, this notebook is amazing!', 'I hate self-referential jokes!']) # inference"
      ],
      "metadata": {
        "id": "YxKdZTYkVWmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a couple things to take note of:\n",
        "\n",
        "First, we gave it a task, \"text-classification\". There are many different [tasks](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.pipeline.task) such as text generation, text classification, and visual tasks. When `pipeline` is instantiated with a task it actually creates a specific pipeline for the task - in this case a [`TextClassificationPipeline`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#transformers.TextClassificationPipeline).\n",
        "\n",
        "Pipelines for different tasks require different arguments when being called. Text-classification pipelines require either a single string or a list of strings when being called. Make sure to check the docs for the specific type of `pipeline`.\n",
        "\n",
        "When we give it a task without specifying a model it defaults to one. For \"text-classification\" it defaults to \"distilbert\", a type of BERT model. If we want something other than the default, we can pass a model name at instantiation: `pipe = pipeline(model=model_name)`\n",
        "\n",
        "Let's look at another example: If I'm speaking with my German friends, I might like to use sentiment classification what they're saying in German. Fortunately, there's a pretrained [model](https://huggingface.co/oliverguhr/german-sentiment-bert) for that!"
      ],
      "metadata": {
        "id": "9sMDCdtYWYo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(model='oliverguhr/german-sentiment-bert') # instantiate with model name\n",
        "pipe('Carter, ich hasse deinen Humor!') # we can run inference with just a string!"
      ],
      "metadata": {
        "id": "GET3yuVMZ2CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yikes, looks bad...\n",
        "\n",
        "Let's turn to a different task, generating text! If we want to generate from the following prompt:\n",
        "\n",
        "`AP News: The University of Washington recently announced`\n",
        "\n",
        "We can create a new type of pipeline!"
      ],
      "metadata": {
        "id": "HmQgKLxKaRk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a text generation pipeline!\n",
        "pipe = pipeline('text-generation')\n",
        "pipe('AP News: The University of Washington recently announced')"
      ],
      "metadata": {
        "id": "eUAe_-zJbriA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our first step under the hood: transformers and tokenizers"
      ],
      "metadata": {
        "id": "Jeex8XRUdIIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good work! Let's dive a bit deeper into what actually happens inside the pipeline!"
      ],
      "metadata": {
        "id": "ax_PAftd6kdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# example pipeline using AutoModel and AutoTokenizer\n",
        "class TextPipe:\n",
        "    def __init__(self, model):\n",
        "        # download models and tokenizers\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    \n",
        "    def __call__(self, prompt):\n",
        "        # make sure it is a list\n",
        "        if type(prompt) is str:\n",
        "            prompt = [prompt]\n",
        "        \n",
        "        # generate \n",
        "        outputs = []\n",
        "        for p in prompt:\n",
        "            # tokenize the prompt\n",
        "            tokenized_prompt = self.tokenizer(p, return_tensors='pt')\n",
        "            # forward pass through the model\n",
        "            gen_tensor = self.model.generate(tokenized_prompt['input_ids'])\n",
        "            # decode the model outputs\n",
        "            gen_text = self.tokenizer.decode(gen_tensor[0])\n",
        "            outputs.append(gen_text)\n",
        "            # note that we could pass everything in a batch, but i want to be explicit\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "lzRC0nI87IZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try it out!\n",
        "\n",
        "pipe = TextPipe(model='gpt2')\n",
        "pipe(['amazon.com is the', 'AI will eventually'])"
      ],
      "metadata": {
        "id": "j2aU0Ng2-3sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two stored fields: `model` and `tokenizer`. `model` comes from: `AutoModelForCausalLM`, a HF class for loading AI models. In this case it loads a pretrained GPT2. `AutoTokenizer` does something similar, loading a tokenizer for GPT2. These AutoThings basically instantiate a class, loading weights or configurations from for them. There are multiple types of AutoThings, but I'll mainly focus on lanugage generation for the rest of this notebook."
      ],
      "metadata": {
        "id": "P5cZao6XDvTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first look at `AutoTokenizer`. This is an object that can encode and decode plaintext into tensors which can be used by models. You need to instanitate it with `AutoTokenizer.from_pretrained(name)`, loading `name`'s associated tokenizer from the HF Hub. Often these will be some form of a GPT2 tokenizer (it is exactly that in this case).\n",
        "\n",
        "There's two important methods you should know: First, simply calling `tokenizer(input)` encodes a string or list of strings. One must specify the flag `return_tensors='pt'` to return PyTorch tensors **THIS IS IMPORTANT**. The output will be a dict containing keys `input_ids` and `attention_mask`. These keys point to PyTorch tensors which can be passed into a model later."
      ],
      "metadata": {
        "id": "HCBgOR0wNzlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2') # load gpt2 tokenizer\n",
        "out = tokenizer('This is an example text', return_tensors='pt') # one example (string)\n",
        "out # returns a dict with input_ids and attention_mask pointing to tensors"
      ],
      "metadata": {
        "id": "OrRpQRgvQg2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass in multiple strings in a list, we need to make sure they're the same length. If they aren't, we'll get an error:"
      ],
      "metadata": {
        "id": "_ccSq_P-RYgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# they must be the same length\n",
        "tokenizer(['This is example one', 'I am example two!'], return_tensors='pt')"
      ],
      "metadata": {
        "id": "7hft13ReRMUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize texts which are different lengths, we need to tell the model how to deal with that. There's two main options - truncate to a certain length, or pad (with special tokens) to a certain length ([docs](https://huggingface.co/docs/transformers/pad_truncation)). For now, I'll pad to the longest sequence in the batch. To do so, I'll need to pass the argument `padding=True`."
      ],
      "metadata": {
        "id": "k74MrXNvRtgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# again, but with padding!\n",
        "tokenizer(['This is example one', 'I am example two!'], return_tensors='pt', padding=True)"
      ],
      "metadata": {
        "id": "_kY4E5AYSkmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shoot, we need to tell the model what token to pad with. A typical choice is the tokenizer's end of sentence or `eos` token. We can set it like this:"
      ],
      "metadata": {
        "id": "TP-aVx2FSoMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token # set pad token to eos token\n",
        "# we try again!\n",
        "out = tokenizer(['This is example one', 'I am example two!'], return_tensors='pt', padding=True)\n",
        "out"
      ],
      "metadata": {
        "id": "vAkFRlA6S7n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It works! Note that the returned tensors now have first dimension of two, because we passed in two inputs."
      ],
      "metadata": {
        "id": "1TJqa77rTJFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out['input_ids'].shape"
      ],
      "metadata": {
        "id": "bqXMKaWKTQd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the second important method: `tokenizer.decode(input)` (and `tokenizer.batch_decode`). We want to be able to decode outputs from the model - this is the method for that!\n",
        "\n",
        "The `input` for `tokenizer.decode(input)` should be a PyTorch tensor of encoded text with **one** dimension. We can first encode text to get a tensor, then decode it and it should be the same!"
      ],
      "metadata": {
        "id": "G8N_RaYfTYaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer('This is example text', return_tensors='pt')\n",
        "\n",
        "# decode text\n",
        "tokenizer.decode(tokenized_text['input_ids'])"
      ],
      "metadata": {
        "id": "QPorq0MkUJi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shoot, I forgot that `tokenizer(input)` outputs a batch dimension always, even if it is one. Let's index into the first dimension and try again."
      ],
      "metadata": {
        "id": "HPHr5RtUU2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer('This is example text', return_tensors='pt')\n",
        "\n",
        "print(tokenized_text['input_ids'].shape)\n",
        "\n",
        "# decode text\n",
        "tokenizer.decode(tokenized_text['input_ids'][0]) # index into first dim this time"
      ],
      "metadata": {
        "id": "kzCF2OgfVLgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweet! What if we want to decode *using* a batch? Well, you guessed it, use: `tokenizer.batch_decode(input)`. This expects a batch dimension - let's give it one!"
      ],
      "metadata": {
        "id": "yQJM1-DKVPer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode text\n",
        "tokenized_text = tokenizer(\n",
        "    ['I am example one!', 'Do not forget about example two!'],\n",
        "    return_tensors='pt',\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "print(tokenized_text['input_ids'].shape)\n",
        "\n",
        "# decode text\n",
        "tokenizer.batch_decode(tokenized_text['input_ids']) # no need to index"
      ],
      "metadata": {
        "id": "4c9pN9SJVvdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! We can even see the `eos` tokens that were used to pad!"
      ],
      "metadata": {
        "id": "r6VN6b9HWAck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForCausalLM` is pretty similar to `AutoTokenizer`. Again, it loads a type of model from HF Hub that you pass in when you instantiate with `AutoModelForCausalLM.from_pretrained(model_name)`. There's also two methods that I'll highlight here as well!\n",
        "\n",
        "The first is simply calling `model()`, running a forward pass of the model. It often requires several parameters:\n",
        "- `input_ids` is a tokenized PyTorch tensor (we saw this from the tokenizer)!\n",
        "- `attention_mask` is another PyTorch tensor, again created with the tokenizer.\n",
        "- `labels` is not always required, but allows the `model` to output a loss as well. For language generation, `labels` typically is the same as `input_ids`.\n",
        "\n",
        "For GPT-2, this will output a data structure containing logits and sometimes a loss.\n",
        "\n",
        "Let's take a look at this in action!"
      ],
      "metadata": {
        "id": "mOa9GnGlWM26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "# tokenize text\n",
        "x = tokenizer('What a great input string!', return_tensors='pt')\n",
        "\n",
        "# forward pass\n",
        "out = model(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
        "out"
      ],
      "metadata": {
        "id": "REPaKIM4X1rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to ignore the wall of text that just appeared and just focus on the first line. For me it is:\n",
        "\n",
        "`CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -37.2172,  -36.8864,  -40.3563,  ...`\n",
        "\n",
        "The model outputs some sort of weird `CausalLMOutputWithCrossAttentions`. IMO this is a bit confusing, but we roll with it. Let's look inside this data structure.\n",
        "\n",
        "First, we have `loss=None` No loss was calculated because we didn't pass it `labels` when it was called. We'll see more about that in a moment.\n",
        "\n",
        "Second, we have the raw `logits`, the next token probabilities for each token in `input_ids`. This is huge because for each input token in `input_ids`, each of the 50,000+ output tokens was given a score.\n",
        "\n",
        "Let's take a look at how we can get loss in out output. To do so, we need to pass `labels` in as well. As mentioned before, labels will be the same as `input_ids`."
      ],
      "metadata": {
        "id": "tH6QGovCZ5I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize text\n",
        "x = tokenizer('What a great input string!', return_tensors='pt')\n",
        "\n",
        "# forward pass\n",
        "out = model(\n",
        "    input_ids=x['input_ids'],\n",
        "    attention_mask=x['attention_mask'],\n",
        "    labels=x['input_ids']\n",
        ")\n",
        "out.loss"
      ],
      "metadata": {
        "id": "EPP20bvUbQnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweet! If we were training, we could now call `out.loss.backward()` and run backpropagation.\n",
        "\n",
        "The second method that is important is `model.generate()`. This allows us to generate text using our model. We can call it without any input, allowing it to ramble on its own!"
      ],
      "metadata": {
        "id": "I3hWSkCnbgGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text\n",
        "model.generate()"
      ],
      "metadata": {
        "id": "qfV8DWpRb3ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right, it outputs a tensor with so we need to decode it using the tokenizer. There's a batch dim, so we should index in!"
      ],
      "metadata": {
        "id": "tyLnJEaecBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text\n",
        "out = model.generate()\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "pKwg1810cA-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! If we want to prompt it, we can encode text then pass the tensor into the model when generating."
      ],
      "metadata": {
        "id": "GS7_A1CLcfMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode prompt\n",
        "prompt = tokenizer('The UW is the best', return_tensors='pt')\n",
        "\n",
        "# generate text\n",
        "out = model.generate(**prompt)\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "FM0TFO2-cm3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect. As we can see, we've been getting warnings about not setting a `max_length` or `max_new_tokens`. We can control text generation via a variety of flags ([docs](https://huggingface.co/docs/transformers/main_classes/text_generation))! For this example, we can focus on how many new tokens to generate.\n",
        "\n",
        "To do so, we can use the `max_new_tokens` and `min_new_tokens` flags."
      ],
      "metadata": {
        "id": "vOKhpbbKc4hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode prompt\n",
        "prompt = tokenizer('The UW is the best', return_tensors='pt')\n",
        "\n",
        "# generate text with 50 new tokens\n",
        "out = model.generate(**prompt, min_new_tokens=50, max_new_tokens=50)\n",
        "\n",
        "# decode output\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "pRU5hMAVdW5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a grasp on this, let's take a look at what it would take to fine-tune our own models!"
      ],
      "metadata": {
        "id": "W5wLqohsdptK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model - loading a dataset\n"
      ],
      "metadata": {
        "id": "2aln1PWNOFaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model, we need data to train on. Fortunately HF has a bunch of datasets on their Hub. (I reread this and it sounded like an ad read. Sorry.) To download a dataset, we can use the `load_dataset` function from the `datasets` library. Lets do so for a dataset on [financial news](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic)."
      ],
      "metadata": {
        "id": "mzZ7jALVeqPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('zeroshot/twitter-financial-news-topic')\n",
        "ds"
      ],
      "metadata": {
        "id": "ccTwpg2Qfbxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train our model to generate tweets that are similar to the dataset. We won't need any of the `label`'s, so we can remove them."
      ],
      "metadata": {
        "id": "8XywxzbDfveP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rem_ds = ds.remove_columns('label')\n",
        "rem_ds"
      ],
      "metadata": {
        "id": "HVKfyF6zgAfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to tokenize the dataset. To do so, we can use `ds.map` to run a function over each example in the dataset."
      ],
      "metadata": {
        "id": "zUFCg-rNgMVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization function\n",
        "def token_func(example):\n",
        "    return tokenizer(example['text'])\n",
        "    \n",
        "# run over entire dataset\n",
        "tokenized_ds = rem_ds.map(token_func)\n",
        "tokenized_ds"
      ],
      "metadata": {
        "id": "dLZHtW3qgXwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We no longer need the `text` column, so we can remove it."
      ],
      "metadata": {
        "id": "KEci4frJgz_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rem_tokenized_ds = tokenized_ds.remove_columns('text')\n",
        "rem_tokenized_ds"
      ],
      "metadata": {
        "id": "4pfM_Bkng9Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we batch texts to be a consistant size (don't worry much about this part). This will reduce our texts down to a small amount of examples because each one was quite short"
      ],
      "metadata": {
        "id": "T7eU2PEGhUoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "# group texts into blocks of block_size\n",
        "block_size = 1024\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "batched_ds = rem_tokenized_ds.map(group_texts, batched=True)\n",
        "batched_ds"
      ],
      "metadata": {
        "id": "QZ2GMEZyhePW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to want a loss, so we will copy the `input_ids` column to the `labels` column as well."
      ],
      "metadata": {
        "id": "UOVEK-RmjHdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ds['train'] = batched_ds['train'].add_column('labels', batched_ds['train']['input_ids'])\n",
        "batched_ds"
      ],
      "metadata": {
        "id": "V2olthqvjG56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can create a standard PyTorch dataloader from these datasets. I'll use HF's `default_data_collator`. We won't do a validation run so we only create `train_dl`."
      ],
      "metadata": {
        "id": "dfMC1KFciZPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    batched_ds['train'],\n",
        "    shuffle=True,\n",
        "    batch_size=2, # small batch size bc i want to ensure it runs\n",
        "    collate_fn=default_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "4keDqpxliYru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can create a standard training loop and train the model for 100 batches! Try making your own edits to this!\n",
        "\n",
        "Note: this will run much faster on a GPU. If you aren't using one, you can switch by pressing \"runtime\", then \"change runtime type\", then specify \"gpu\". Note that this will restart your current Colab session."
      ],
      "metadata": {
        "id": "DSwRdWGgi-eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "dl_iter = iter(train_dl)\n",
        "\n",
        "#for batch in train_dl: # uncomment to run full epoch\n",
        "for i in range(100):\n",
        "    batch = next(dl_iter)\n",
        "    # push all to device\n",
        "    batch = {k: batch[k].to(device) for k in batch.keys()}\n",
        "    # forward pass\n",
        "    out = model(**batch)\n",
        "    optimizer.zero_grad()\n",
        "    out.loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "TrAdHBpFkBM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what our model now produces!"
      ],
      "metadata": {
        "id": "rUmPAjtnoalt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text\n",
        "out = model.generate(min_new_tokens=30, max_new_tokens=30)\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "id": "puwzYWXVoc_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It needs more training, but you can see that it is starting to learn!\n",
        "\n",
        "To upload the model, we can use `model.push_to_hub()`. We first need to login to HF using the cli command `huggingface-cli login`."
      ],
      "metadata": {
        "id": "ITLtPS2lp7KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub('username/test_model')"
      ],
      "metadata": {
        "id": "JSFxCsPrtaPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for your time! Let me know if you have any feedback!"
      ],
      "metadata": {
        "id": "iMnbP7dqttOL"
      }
    }
  ]
}