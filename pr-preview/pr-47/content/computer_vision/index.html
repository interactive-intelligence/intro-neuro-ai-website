
<!DOCTYPE html>
<html lang="en-US">
<head>
 <meta charset="UTF-8">
 <meta http-equiv="X-UA-Compatible" content="IE=Edge">
<link rel="stylesheet" href="/pr-preview/pr-47/assets/css/just-the-docs-default.css">
    <script src="/pr-preview/pr-47/assets/js/vendor/lunr.min.js"></script>
  <script src="/pr-preview/pr-47/assets/js/just-the-docs.js"></script>
 <meta name="viewport" content="width=device-width, initial-scale=1">
<title>Computer Vision | I2 Intro Neuro/AI</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Computer Vision" />
<meta name="author" content="Interactive Intelligence" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<link rel="canonical" href="https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-47/content/computer_vision/" />
<meta property="og:url" content="https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-47/content/computer_vision/" />
<meta property="og:site_name" content="I2 Intro Neuro/AI" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Computer Vision" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Computer Vision","url":"https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-47/content/computer_vision/"}</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true },
          { left: '$', right: '$', display: false },
          { left: '\\(', right: '\\)', display: false },
          { left: '\\[', right: '\\]', display: true }
        ],
        // • rendering keys, e.g.:
        throwOnError: false
      });
    });
  </script>
<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
 <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
   <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
 <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
   <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
 <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
   <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
 <title id="svg-external-link-title">(external link)</title>
 <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
 <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
   <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
 <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>
<symbol id="svg-copy" viewBox="0 0 16 16">
 <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
   <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
 <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
   <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>
</svg>
 <div class="side-bar">
 <div class="site-header">
    <a href="/pr-preview/pr-47/" class="site-title lh-tight">
  I2 Intro Neuro/AI
</a>
    <a href="#" id="menu-button" class="site-button">
      <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
    </a>
 </div>
 <nav aria-label="Main" id="site-nav" class="site-nav">
     <ul class="nav-list"><li class="nav-list-item"><a href="/pr-preview/pr-47/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item"><a href="/pr-preview/pr-47/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Core Content category">
            <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg>
          </a><a href="/pr-preview/pr-47/content/" class="nav-list-link">Core Content</a><ul class="nav-list"><li class="nav-list-item "><a href="/pr-preview/pr-47/content/machine_learning/" class="nav-list-link">Machine Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/deep_learning/" class="nav-list-link">Deep Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/basic_neuro/" class="nav-list-link">Basic Neuroanatomy</a><li class="nav-list-item  active"><a href="/pr-preview/pr-47/content/computer_vision/" class="nav-list-link active">Computer Vision</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/visual_system/" class="nav-list-link">Visual System</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/reinforcement_learning/" class="nav-list-link">Reinforcement Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/ai_ethics/" class="nav-list-link">AI Ethics</a><li class="nav-list-item "><a href="/pr-preview/pr-47/content/language_modeling/" class="nav-list-link">Language Modeling</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander" aria-label="toggle links in Additional Content category">
            <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg>
          </a><a href="/pr-preview/pr-47/addcontent/" class="nav-list-link">Additional Content</a><ul class="nav-list"><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/build_a_brain/" class="nav-list-link">Build a Brain</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/cogsci_pain/" class="nav-list-link">Deinforcement Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/fairness_theory/" class="nav-list-link">Fairness & Theory</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/gradient_descent/" class="nav-list-link">Gradient Descent</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/human_char_brain/" class="nav-list-link">Human Brain Characteristics</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/movement/" class="nav-list-link">Movement</a><li class="nav-list-item "><a href="/pr-preview/pr-47/addcontent/creative_neuro/" class="nav-list-link">Neuroscience & Creativity</a></ul><li class="nav-list-item"><a href="/pr-preview/pr-47/staff/" class="nav-list-link">Course Staff</a><li class="nav-list-item"><a href="/pr-preview/pr-47/graduates/" class="nav-list-link">Graduates</a></ul>
 </nav>
   <footer class="site-footer">
      This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
   </footer>
</div>
 <div class="main" id="top">
   <div id="main-header" class="main-header">
<div class="search">
 <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
 </div>
 <div id="search-results" class="search-results"></div>
</div>
   <nav aria-label="Auxiliary" class="aux-nav">
 <ul class="aux-nav-list">
     <li class="aux-nav-list-item">
        <a href="https://interactive-intelligence.github.io" class="site-button"
        >
          UW Interactive Intelligence Website
        </a>
 </ul>
</nav>
</div>
   <div id="main-content-wrap" class="main-content-wrap">
   <nav aria-label="Breadcrumb" class="breadcrumb-nav">
     <ol class="breadcrumb-nav-list">
         <li class="breadcrumb-nav-list-item"><a href="/pr-preview/pr-47/content/">Core Content</a>
       <li class="breadcrumb-nav-list-item"><span>Computer Vision</span>
     </ol>
   </nav>
     <div id="main-content" class="main-content" role="main">
         <h1 id="computer-vision">
    <a href="#computer-vision" class="anchor-heading" aria-labelledby="computer-vision"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Computer Vision</u>
</h1>
<p>Hello and welcome to the <em>Computer Vision</em> section of the I2 course! Here we will approach CV from a deep learning perspective, to connect better with other units.</p>
<h2 id="technical-track-content">
    <a href="#technical-track-content" class="anchor-heading" aria-labelledby="technical-track-content"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Technical Track Content</u>
</h2>
<h3 id="task-1">
    <a href="#task-1" class="anchor-heading" aria-labelledby="task-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 1:</u></strong>
</h3>
<p><em>Navigate to the relevant section of the I2 Grimoire using the link below. Read the textbook and answer all synthesis questions to the best of your ability. Be sure to save these somewhere for future reference.</em></p>
<h3 id="i2-grimoire-computer-vision">
    <a href="#i2-grimoire-computer-vision" class="anchor-heading" aria-labelledby="i2-grimoire-computer-vision"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://grimoire.uw-i2.org/units/Computer%20Vision.pdf">I2 Grimoire: Computer Vision</a>
</h3>
   <hr />
<h3 id="task-2">
    <a href="#task-2" class="anchor-heading" aria-labelledby="task-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 2:</u></strong>
</h3>
<p><em>Solve the coding challenges within the Jupyter notebook linked below (through Colab). If you encounter any issues with the notebook not functioning as described, please let us know!</em></p>
<p>Please ask questions as you work through this project. Be sure to discuss with others in your group if you have one! Share your answers as you like, the goal is to learn and we’re not holding grades over your head.</p>
<p>In this project, you will be implementing a Convolutional Neural Network (CNN)! I would suggest to <strong>read up on what <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10</a> is.</strong> before starting since this is the data you will be working with.</p>
<p><strong>Colab Link:</strong> <a href="https://colab.research.google.com/github/interactive-intelligence/intro-neuro-ai-website/blob/main/notebooks/unit-04/conv-net.ipynb">Computer Vision Colab Notebook</a> <strong>(1 hr)</strong></p>
<p>When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas.</p>
<p>Remember that this is all for your learning, so do your best and don’t stress!</p>
<p>Congratulations! You now understand the basics of Convolutional Neural networks!</p><hr /><hr /><hr />
<h2 id="literacy-track-content">
    <a href="#literacy-track-content" class="anchor-heading" aria-labelledby="literacy-track-content"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Literacy Track Content</u>
</h2>
<h3 id="task-1-1">
    <a href="#task-1-1" class="anchor-heading" aria-labelledby="task-1-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 1:</u></strong>
</h3>
<p><em>Read the article below, and answer any synthesis questions placed along the way.</em></p>
<p>In the deep learning unit, we talked about how deep neural networks work on a very general level. Today, we’re going to talk about a new type of neural network, called a Convolutional Neural Network (CNN). These are neural networks that are specifically designed to process images. While neural networks resemble the way the human brain works, CNNs resemble the way the human vision system works. Often, a CNN is a subsection of a larger neural network: it’s like a group of layers that exists to process images really efficiently.</p>
<p>Let’s think back to our discussion of neural networks in unit 2. Recall that we used the MNIST dataset, which contains images of handwritten numbers (here’s a picture to remind you, with <a href="https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe">credit</a>):</p>
<div style="text-align:center">
    <img src="../assets/unit2/literacy_images/labeled_mnist.webp" alt="Labeled images from the MNIST database" width="500" />
</div>
<p>Near the end of the article, we mentioned that as we kept training our neural network, the computer would begin to pick out patterns associated with each number (like ones usually being a straight vertical line, or zeroes being a circle). But how exactly does the computer figure that out? How does the neural network “realize” what a one, a zero, or any other number looks like?</p>
<p>This is where the <strong>convolutional layer</strong> comes in. A CNN uses its convolutional layers to identify patterns and utilize them to analyze images. A convolutional layer would pick out the features that distinguish one number from the others.</p>
<p>Recall the structure of a general neural network.</p>
<div style="text-align:center">
    <img src="../assets/unit2/literacy_images/neural_network.png" alt="Diagram of a neural network" width="500" />
</div>
<p>Notice that every single node in the input layer is connected to every single node in the hidden layer. Remember, every node in the input layer corresponds to a pixel in the image. Our pictures of handwritten numbers contain 784 pixels (28 x 28), which means there are 784 nodes in the input layer. This means we have a lot of connections between the input layer and the first hidden layer, and the first hidden layer and the second hidden layer, and so on.</p>
<p>The reason the neural network is set up like this is because it assumes that information about every pixel influences every other pixel. But that’s not necessarily true! For example, in each image in the MNIST dataset, the dark pixels tend to be close to other dark pixels, and the light pixels tend to be close to other light pixels.</p>
<p>Also, the patterns associated with a given number are not exactly the same every time. Take a look at the image below (<a href="https://towardsdatascience.com/part-5-training-the-network-to-read-handwritten-digits-c2288f1a2de3">credit</a>) and notice how there are slight differences in the way the numbers are written (e.g. the number 7 vs. 7 with a line through it). We need the computer to recognize that these are just variations on the exact same number.</p>
<div style="text-align:center"> 
    <img src="../assets/unit4/literacy_images/various_numbers.png" alt="Various numbers in the MNIST dataset" width="500" />
</div>
<p>CNNs account for both of these things, reducing the number of input nodes and allowing for variations in the pixels when identifying features in an image.</p>
<p>Now we’re going to get into how CNNs actually work, using a very simple image of a handwritten number.</p>
<p>Take a look at the image below, which shows the number 8. We’ve assigned a numerical value to each pixel, like was discussed in Unit 2.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/cnn_sample_8.png" alt="Sample image showing the number 8, with pixels labeled and numbered" width="300" />
</div>
<p>The first thing a CNN does is use filters to identify the locations of features in the image. In our example, the most important feature of an 8 is an ellipse, because an 8 is made of two ellipses stacked on top of each other. So let’s use a filter to identify any ellipses in the image. At first, our filter just contains random numbers—but through backpropagation, it starts to resemble the shape of the feature we’re looking for (an ellipse). Below is an example of what the correct filter would look like.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/cnn_filter.png" alt="Sample filter for above image" width="300" />
</div>
<p>Next, we put the filter over the image and compute the <strong>dot product</strong> of the filter and the portion of the image that it overlaps. This involves multiplying the value in the filter by the value of the pixel underneath, and adding up all the products.</p>
<p>Take a look at the example below. Notice that the feature in the filter (the ellipse) doesn’t perfectly match up with the pixels underneath it, so our dot product ends up being 4.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/conv1.png" alt="Diagram of first convolution" width="500" />
</div>
<p>Then, we shift our filter over by one and do the same thing. The distance by which we move our filter is called the <strong>stride</strong>. Notice that in this next example, the feature in the filter perfectly matches with the pixels underneath, so our dot product is much larger (8).</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/conv2.png" alt="Diagram of second convolution" width="500" />
</div>
<p>We’ll continue shifting the filter until we’ve covered the entire image (there will probably be a lot of overlap). We keep track of all these dot products in a <strong>feature map</strong>. The feature map is useful because it tells us the general locations of the feature we’re tracking. Our feature map is a 3 by 3 matrix because we apply the filter nine times overall (you can try shifting the filter and calculating the dot products yourself!).</p>
<p>Remember, we said that when the filter perfectly matched the pixels underneath (i.e., there was an ellipse in the image), the dot product was 8. The feature map has two 8s, which tells us that there are two ellipses in the image: in the top middle of the image and in the bottom middle of the image. This fits with what we know an 8 should look like: two ellipses stacked on top of each other!</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/feature_map.png" alt="Diagram of feature map" width="500" />
</div>
<p>The entire process we just did, of converting an image into a feature map, is called a <strong>convolution</strong>.</p>
<p>The image we’re using, again, is really simple: it only has one distinguishing feature, which is its ellipses. If our image has multiple features (e.g. multiple lines, angles, edges, curves, etc.), we would use a different feature map for each one, and do convolutions for each feature all at the same time!</p>
<p>Next, we’re going to simplify this information a little bit using a strategy called <strong>pooling</strong>. Pooling means to reduce our feature map into an even smaller matrix that contains the most important information from each “region” of the image. Our image is a little too simple to pool any further, but below is an example of how that would work (<a href="https://paperswithcode.com/method/max-pooling">credit</a>). Here, the entire top left corner is simplified to just the largest value, and so on for the other regions. This gives us a simplified understanding of the feature map overall.</p>
<p>The example below uses “max pooling,” which means it takes the largest value from each region to represent the region as a whole. There are other types of pooling as well, such as “average pooling,” which takes the average of the region to represent the overall region.</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/pooling.png" alt="Example of max pooling" width="500" />
</div>
<p>We repeat the convolution-pooling cycle until our feature map (or maps, if we have multiple features) is sufficiently small. Finally, we plug our resulting feature maps into a <strong>fully connected layer</strong>, which is like a regular old neural network. Before, all our features were analyzed independently in different convolutions. Here, we’re putting everything together and using all our collected information to classify our image. We do this by taking all of our small, pooled feature maps, flattening them into a column vector, and treating this column vector as the <strong>input layer</strong> to a standard neural network, which then predicts what the final image is. Take a look at the image below for an example (<a href="https://slds-lmu.github.io/seminar_nlp_ss20/convolutional-neural-networks-and-their-applications-in-nlp.html">credit</a>).</p>
<div style="text-align:center">
    <img src="../assets/unit4/literacy_images/fully_connected.png" alt="Example of fully connected layer" width="500" />
</div>
<p>This was a lot of information, so please reach out to someone if you’re having trouble with these concepts! Now lets watch the following videos and answer the associated questions.</p>
<h3 id="video-1-how-convolutional-neural-networks-work-12-min">
    <a href="#video-1-how-convolutional-neural-networks-work-12-min" class="anchor-heading" aria-labelledby="video-1-how-convolutional-neural-networks-work-12-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 1:</strong> <a href="https://youtu.be/FmpDIaiMIeA?t=840">How Convolutional Neural Networks work</a> <strong>(12 min)</strong>
</h3>
<p><strong>Note:</strong> Watch from 13:54 onward to answer the questions below. Before that is mainly review from this article (but you may find it useful to skim through, as it covers helpful math concepts!).</p>
<div class="center">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/FmpDIaiMIeA?si=NsjD1QZB_v7itFpK&amp;start=834" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>
<h3 id="synthesis-questions">
    <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><code class="language-plaintext highlighter-rouge">How is backpropagation used in CNNs, and how does it differ from backpropagation in standard neural networks?</code>
 <li><code class="language-plaintext highlighter-rouge">What outcomes can a designer achieve from adjusting the hyperparameters or architecture of a CNN?</code>
 <li><code class="language-plaintext highlighter-rouge">Can you think of an example of when we can use a CNN on non-image data?</code>
</ul>
<h3 id="video-2-but-what-is-a-convolution-14-min">
    <a href="#video-2-but-what-is-a-convolution-14-min" class="anchor-heading" aria-labelledby="video-2-but-what-is-a-convolution-14-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 2:</strong> <a href="https://www.youtube.com/watch?v=KuXjwB4LzSA">But what is a convolution?</a> <strong>(14 min)</strong>
</h3>
<p><strong>Note:</strong> Watch up to 13:42 in this video; the rest is beyond the scope of this course.</p>
<div class="center">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/KuXjwB4LzSA?si=GTq2Z7DT4y9WyF6E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>
<h3 id="synthesis-questions-1">
    <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><code class="language-plaintext highlighter-rouge">What is the name for the smaller grid that convolves over a larger image?</code>
   <ul>
     <li><code class="language-plaintext highlighter-rouge">Hint: Starts with a "k"</code>
   </ul>
 <li><code class="language-plaintext highlighter-rouge">What are some examples of what you can do to images if you convolve them with special matrices?</code>
 <li><code class="language-plaintext highlighter-rouge">How does Gaussian blur "work"?</code>
 <li><code class="language-plaintext highlighter-rouge">What is the name for the actual operation that occurs when the smaller grid is overlaid on the larger one?</code>
   <ul>
     <li><code class="language-plaintext highlighter-rouge">When each element of the corresponding pixels are multiplied then summed.</code>
   </ul>
 <li><code class="language-plaintext highlighter-rouge">Give an example of a 3x3 matrix that would not do anything to the image it convolves over. Why does it not impact the image?</code>
   <ul>
     <li><code class="language-plaintext highlighter-rouge">This is also known as the "do-nothing" matrix</code>
   </ul>
</ul><hr />
<h3 id="task-2-1">
    <a href="#task-2-1" class="anchor-heading" aria-labelledby="task-2-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 2:</u></strong>
</h3>
<p><em>Complete the following writing activity.</em></p>
<p>The non-technical project for this unit will involve some writing! <strong>Choose 3</strong> of the prompts below and write <strong>at least 200</strong> (<em>meaningful!</em>) words on each one! We will not be strictly grading you on correctness or anything like that. This is an opportunity to deeply engage with the material you have just learned about, and creatively connect it to neuroscience!</p>
<ul>
 <li>How are CNNs inspired by the human visual system?
 <li>What are some similarities and differences between CNNs and the human visual system?
 <li>How is the pooling layer in CNNs related to the brain’s visual processing?
 <li>What ways does the convolutional layer in CNNs resemble the receptive field in the visual system?
 <li>Reflecting on you have learned from this unit, what is one thing you found to be most interesting?
 <li>What is one concept from this unit that you would like to learn more about and why?
</ul>
     </div>
   </div>
<div class="search-overlay"></div>
 </div>
  
