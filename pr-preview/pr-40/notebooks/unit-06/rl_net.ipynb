{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Project!\n",
    "\n",
    "##### How to work through this project:\n",
    "- Go cell by cell and finish the marked #TODO's\n",
    "- You don't need to touch the code marked between the `#---------#`. Those are puzzle pieces that your code will fit into!\n",
    "    - However, I **STRONGLY** encourage you to understand every single line between those blocks. They are essential!\n",
    "    - It is crucial that your variable names are what we expect them to be, or the puzzle pieces won't fit.\n",
    "- Tutorials/helpful information will be placed in the `.md` cells above the \"work\" cells. Consult them if you are stuck.\n",
    "- If you REALLY cannot find the correct code to make the cell run, consult the `[proj]-ans.ipynb`.\n",
    "- The final product (what we expect to see if you run all the cells consecutively) will be placed in the `answers/` directory.\n",
    "    - Chances are your output won't be the exact same (stochasticity!) but it should be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get used to these imports!\n",
    "#----------------------------------------------------------------#\n",
    "#To install: pip install numpy\n",
    "import numpy as np \n",
    "#To install: pip install matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "#To install: pip install torch (not GPU compatible)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#To install: pip install gymnasium[classic_control]\n",
    "import gymnasium as gym\n",
    "\n",
    "# No pip install needed\n",
    "import random\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, clear_output\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be implementing DQN for Cartpole, which is a common RL benchmark from OpenAI! First, lets visualize the environment that our Deep Q-Learnign Network will operate in. You can read more about cartpole [here](https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288)\n",
    "\n",
    "An excerpt is provided below if you would like to understand the dynamics of the system:\n",
    "\n",
    "> ##### A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Run the following code to visualize the system dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "env.reset()\n",
    "\n",
    "num_steps_to_viz = 200\n",
    "step_count = 0\n",
    "for i in range(num_steps_to_viz):\n",
    "   step_count += 1\n",
    "   if(i % 3 == 0): # Speed up the framerate\n",
    "      plt.imshow(env.render())\n",
    "      display(plt.gcf())    \n",
    "      clear_output(wait=True)\n",
    "\n",
    "   _, _, terminated, truncated, _ =  env.step(random.randrange(0,2)) # take a random action\n",
    "   done = terminated or truncated\n",
    "   if done: \n",
    "      print(\"Pole tipped too far\")\n",
    "      print(f\"Survived for {step_count} steps\")\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the states represent (from the environment) and what the actions do (given to the environment)\n",
    "\n",
    "**Observation** - Type: Box(4)\n",
    "- Index 0: Cart Position - [-4.8, 4.8]\n",
    "- Index 1: Cart Velocity - (-inf, inf)\n",
    "- Index 2: Pole Angle - [-24 deg, 24 deg]\n",
    "- Index 3: Pole Velocity At Tip - (-inf, inf)\n",
    "\n",
    "**Action** - Type: Discrete(2)\n",
    "- 0: Push cart to the left\n",
    "- 1: Push cart to the right\n",
    "\n",
    "These states are returned as some weird datatypes, but a little modification can easily turn them into tensors *(you will find this a common issue, turning things to/from tensors)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create the Deep Q-Learning Network (DQN) that will play cartpole for us. If you remember from the lesson, Q-learning is powerful because it allows for \"experience replay\" where transitions can be saved inside of a memory buffer and then \"re-experienced\" by the model to learn. Note that technically this is not the Q-function as it only operates on the state and outputes an action. **This is more analogous to a raw policy.**\n",
    "\n",
    "Take a look at your older projects for a refresher on how to make a neural network. The specification for the network itself is detailed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a DQN \n",
    "## TODO: Create a neural network called DQN that has variable input and output layer sizes (input variable should be state_dim, output is action_dim). \n",
    "## TODO: It should have 2+ layers with relu activations for all but the last layer. Also, the hidden layer sizes should all be 128!\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Hidden layers don't always have to be smaller than the layer before it! Especially if the input dimensionality is small\n",
    "- Remind yourself what activations are and why they are useful.\n",
    "- Make sure to name your class \"DQN\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have defined a \"named tuple\" that will store a state \"transition\". Essentially it will hold a previous state (`state_a`) an action taken on that state (`action`), the resultant state from taking that state-action pair (`state_b`) and the reward for taking that state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Creates a named tuple that we can add to\n",
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    ('state_a', 'action', 'state_b', 'reward')\n",
    ")\n",
    "\n",
    "# Example of creating a named tuple\n",
    "\n",
    "t = Transition([0,0,0,0], 1, [1,1], 0.5)\n",
    "\n",
    "# You can check the value of, say, \"action\" by printing t.action\n",
    "\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be implementing a memory storage class that will be accessed in the training loop to \"replay\" memories to make the model better. What is nice about Q-learning is that \"optimization is almost always performed off-policy, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained.\" ([Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)). You can read more about Replay buffers [here](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits)! Here are some important excerpts:\n",
    ">More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.\n",
    "\n",
    ">Better convergence behaviour when training a function approximator. Partly this is because the data is more like i.i.d. data assumed in most supervised learning convergence proofs. \n",
    "\n",
    "Implement the following class. The spec has been written for you. You can read up on the deque class [here](https://docs.python.org/3/library/collections.html#collections.deque). It is the data structure we use to store Transition tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Make a memory buffer storage object\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Read up on how to use the deque object\n",
    "- The random.sample() method may help you here\n",
    "\"\"\"\n",
    "\n",
    "class TransitionMemoryStorage():\n",
    "    \"\"\"A class to hold a buffer of transition tuples that can be sampled from to run experience replay on a DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Creates a buffer to hold transition tuples\n",
    "\n",
    "        Args:\n",
    "            capacity (int): How many elements the buffer can hold at a time\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_transition(self, t):\n",
    "        \"\"\"Adds a transition to the buffer\n",
    "\n",
    "        Args:\n",
    "            t (tuple): A Transition tuple\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"Selects num_samples unique samples from the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to pull\n",
    "\n",
    "        Returns:\n",
    "            list: Sample list of transitions from the buffer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def can_sample(self, num_samples):\n",
    "        \"\"\"Checks if there are at least num_samples samples in the buffer\n",
    "\n",
    "        Args:\n",
    "            num_samples (int): How many samples to check validity for\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the TransitionMemoryStorage object can be sampled on\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement an explore/exploit strategy called \"epsilon-greediness\". This was explained in the megadoc but if you need a refresher check out this link [here](https://www.baeldung.com/cs/epsilon-greedy-q-learning#:~:text=The%20epsilon%2Dgreedy%20approach%20selects,what%20we%20have%20already%20learned.) and read section 5.2. We will use this class when deciding wether to explore (take a random action) or exploit (use the model and hope it knows what it's doign by then)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Look up ways the epsilon-greedy equation has been implemented and see which one works best here\n",
    "- The `should_explore` method should have an <epsilon> probability of returning True where <epsilon> is the epsilon value at `current_step`\n",
    "- Use random.random()\n",
    "\"\"\"\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"Strategy for use in an agent, implements exponential decay epsilon greedy\n",
    "    \"\"\"\n",
    "    def __init__(self, max_epsilon, min_epsilon, decay):\n",
    "        \"\"\"Initializes an Epsilon Greedy strategy\n",
    "\n",
    "        Args:\n",
    "            max_epsilon (float): The initial epsilon value\n",
    "            min_epsilon (float): The ending epsilon value\n",
    "            decay (float): The rate at which the epsilon value will decay\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def should_explore(self, current_step):\n",
    "        \"\"\"Returns True if, according to this strategy at the current timestep, the agent should explore and False otherwise\n",
    "\n",
    "        Args:\n",
    "            current_step (int): How many steps the agent has taken (persists through episodes and failures)\n",
    "\n",
    "        Returns:\n",
    "            boolean: If the agent should explore (take a random action)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _get_explore_prob(self, current_step):\n",
    "        \"\"\"Returns the epsilon value at a certain timestep\n",
    "\n",
    "        Args:\n",
    "            current_step (int): The number of steps that the agent has taken\n",
    "\n",
    "        Returns:\n",
    "            float: Epsilon value at this current timestep\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the Agent tht will actually interact with the environment. Read the tips carefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Create an Agent class that can navigate the environment\n",
    "## TODO: Implement the following class according to the spec\n",
    "\"\"\"\n",
    "Tips:\n",
    "- Be sure to increment the step counter whenever an action is selected\n",
    "- If the strategy says to explore, select a random action. Otherwise, use the model!\n",
    "- The model gives you probabilities (i.e. [0.74, 0.26]) saying which action it thinks is better. Use this information to choose an action.\n",
    "- Scroll up to remind yourself what integers are valid actions (and therefore return values for `select_action`)\n",
    "\"\"\"\n",
    "class Agent():\n",
    "    \"\"\"Agent that acts within the environment using a dqn policy\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy):\n",
    "        \"\"\"Initializes the agent with a strategy for explore vs. exploit\n",
    "\n",
    "        Args:\n",
    "            strategy: Strategy object that dictates to explore or exploit\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.current_step = 0\n",
    "\n",
    "    def select_action(self, input_state, policy_dqn):\n",
    "        \"\"\"Selects an action based on a state and a policy network\n",
    "\n",
    "        Args:\n",
    "            input_state: The state to select an action based on\n",
    "            policy_dqn: Policy network that outputs probabilities to take actions within the action space\n",
    "\n",
    "        Returns:\n",
    "            int: Action to take based on state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to the training loop. This is hard and requires some specific lines of code. Some is pulled from [this](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) tutorial so check it out if you are stuck.\n",
    "Do the following:\n",
    "- Run the code! Convince yourself that it works\n",
    "- Understand how the theory we learnt is used in the `optimize_model` method. Where is the bellman equation used? \n",
    "- Mess with some parameters. How sensitive is this model?\n",
    "\n",
    "We use two networks, the policy and the target network, with a slow update on the target network to match the policy one (literally slowly shifting its weights to match the policy net). This \"lagging self-tuning\" ensures the model wont be caught in a self-optimization loop. After all, DQN is based on optimizing the Bellman equation, not necessarily towards maximizing reward!\n",
    "\n",
    "The `num_episodes` parameter may have to be tweaked. Sometimes, the DQN takes 300 steps to converge to a good solution. Sometimes it takes 1000. This is the problem with DQN's, but I'm sure you will be able to get it to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "from itertools import count\n",
    "# Instantiate an EpsilonGreedyStrategy class\n",
    "e_greedy_strategy = EpsilonGreedyStrategy(max_epsilon=0.9, min_epsilon=0.05, decay=0.99)\n",
    "\n",
    "# Instantiate an agent with the above explore/exploit strategy\n",
    "agent = Agent(strategy=e_greedy_strategy)\n",
    "\n",
    "# Create a TransitionMemoryStorage object\n",
    "memory = TransitionMemoryStorage(1000)\n",
    "\n",
    "# Create a policy and target net\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "\n",
    "# Set up an optimizer and loss function\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "# This is Huber loss, The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large.\n",
    "loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99 # Discount factor\n",
    "TAU = 0.005 # Target network soft update factor\n",
    "\n",
    "def optimize_model():\n",
    "    # If the amount of Transitions saved in memory is not big enough to get a batch from, don't sample\n",
    "    if not memory.can_sample(BATCH_SIZE):\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Batching data for easier processing\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.state_b)), dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.state_b if s is not None])\n",
    "    state_batch = torch.cat(batch.state_a)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    ## Recall the Bellman Equation: Q(s, a) = r + (GAMMA * Q(s', a)) where a is the action selected by the agent\n",
    "    ## The loss we are trying to compute (a.k.a TD-error) is: loss = Q(s, a) - [r + (GAMMA * Q(s', a))]\n",
    "\n",
    "    # This is Q(s, a), the current Q-values\n",
    "    predicted_q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # These lines calculate Q(s', a) where a is the action selected by the agent\n",
    "    next_state_q_values = torch.zeros(BATCH_SIZE)\n",
    "    with torch.no_grad(): # This is needed because we don't want to pass in \"None\" values to our target_net as it would crash\n",
    "        next_state_q_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    # This is r + (GAMMA * Q(s', a))\n",
    "    expected_q_values = reward_batch + (next_state_q_values * GAMMA) \n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = loss_func(predicted_q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model (standard procedure)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def update_target(policy_net, target_net):\n",
    "    \"\"\" Soft update of the target network's weights. target_net ← (policy_net*τ) + [target_net*(1-τ)]\n",
    "    Args:\n",
    "        policy_net : Policy DQN used to select actions\n",
    "        target_net : Target DQN used to prevent self-optimization loop\n",
    "    \"\"\"\n",
    "    target_net_state_dict = target_net.state_dict()\n",
    "    policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "    for key in policy_net_state_dict:\n",
    "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\n",
    "    target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "num_episodes = 500 # Number of episodes to run, can be between 200 to 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    if(i_episode % 25 == 0): # Sanity check every 25 episodes\n",
    "        print(f\"On episode {i_episode}\")\n",
    "\n",
    "    # Get the initial state of the environment\n",
    "    current_state, _ = env.reset() \n",
    "    # Reshape into a tensor of shape (1, 4) which is necessary for batching\n",
    "    current_state = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0) \n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Select an action\n",
    "        action = agent.select_action(current_state, policy_net) \n",
    "        # Take a step in the environment and get the results from that step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action) \n",
    "\n",
    "        # We do not want to consider terminated transition states. This is used to mask data for model updates\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "\n",
    "        # Store the transition in memory, reshaping for batching\n",
    "        memory.add_transition(Transition(current_state, torch.tensor([action]).unsqueeze(0), next_state, torch.tensor([reward])))\n",
    "\n",
    "        # Move to the next state\n",
    "        current_state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        update_target(policy_net, target_net)\n",
    "\n",
    "        # Terminated means the agent \"wins\" or \"loses\", truncated means some other failure happened. Either way, we move on\n",
    "        done = terminated or truncated \n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to test your model! The following method runs a few episodes and tells you the highest # of steps the agent was able to stay alive for across those episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "num_trials = 10000\n",
    "overall_max_reward = 0\n",
    "\n",
    "done = False\n",
    "for i in range(num_trials):\n",
    "    episode_reward = 0\n",
    "    for t in count():\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        state,reward,terminated,truncated,_ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward+=1\n",
    "        overall_max_reward = max(episode_reward, overall_max_reward) \n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(f\"Longest time alive across {num_trials} trials: {overall_max_reward}\")\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method visualizes one epsiode so you can see it! it will be slow since we are rendering individual frames, but you can see how much the agent has improved from the first time we visualized the model and took random steps! You will receive a letter grade based on how well your agend did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "env.reset()\n",
    "\n",
    "step_count = 0\n",
    "for t in count():\n",
    "    step_count += 1\n",
    "    if(t % 10 == 0):\n",
    "        plt.imshow(env.render())\n",
    "        display(plt.gcf())    \n",
    "        clear_output(wait=True)\n",
    "\n",
    "    action = agent.select_action(state, policy_net)\n",
    "    state,reward,terminated,truncated,_ = env.step(action)\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        print(f\"Survived for {step_count} steps!\")\n",
    "        if(step_count <= 50):\n",
    "            print(\"Grade: F\")\n",
    "        elif((step_count > 50) and (step_count <= 100)):\n",
    "            print(\"Grade: D\")\n",
    "        elif((step_count > 100) and (step_count <= 195)):\n",
    "            print(\"Grade: C\")\n",
    "        elif((step_count > 195) and (step_count <= 450)):\n",
    "            print(\"Grade: B\")\n",
    "        elif((step_count > 450) and (step_count <= 500)):\n",
    "            print(\"Grade: A\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"Complete!\")\n",
    "#----------------------------------------------------------------#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions:\n",
    "- Change up the network and other hyperparameters\n",
    "- Plot reward through time using matplotlib.pyplot\n",
    "- Plot moving average of reward using matplotlib.pyplot\n",
    "- Implement A2C (requires research)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on completing the project! Check your results with the notebook in the `answers` directory and then send your final accuracy to your club/channel/mentor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
