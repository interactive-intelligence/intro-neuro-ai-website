
<!DOCTYPE html>
<html lang="en-US">
<head>
 <meta charset="UTF-8">
 <meta http-equiv="X-UA-Compatible" content="IE=Edge">
<link rel="stylesheet" href="/pr-preview/pr-45/assets/css/just-the-docs-default.css">
    <script src="/pr-preview/pr-45/assets/js/vendor/lunr.min.js"></script>
  <script src="/pr-preview/pr-45/assets/js/just-the-docs.js"></script>
 <meta name="viewport" content="width=device-width, initial-scale=1">
<title>Reinforcement Learning | I2 Intro Neuro/AI</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Reinforcement Learning" />
<meta name="author" content="Interactive Intelligence" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<meta property="og:description" content="Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!" />
<link rel="canonical" href="https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-45/content/reinforcement_learning/" />
<meta property="og:url" content="https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-45/content/reinforcement_learning/" />
<meta property="og:site_name" content="I2 Intro Neuro/AI" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Interactive Intelligence"},"description":"Welcome to Interactive Intelligence’s Introduction to Neuro/AI crash course!","headline":"Reinforcement Learning","url":"https://intro-neuro-ai-website-pr-preview.onrender.com/pr-preview/pr-45/content/reinforcement_learning/"}</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css"
    integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js"
    integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
          { left: '$$', right: '$$', display: true },
          { left: '$', right: '$', display: false },
          { left: '\\(', right: '\\)', display: false },
          { left: '\\[', right: '\\]', display: true }
        ],
        // • rendering keys, e.g.:
        throwOnError: false
      });
    });
  </script>
<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
 <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
   <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
 <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
   <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
 <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
   <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
 <title id="svg-external-link-title">(external link)</title>
 <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>
    <symbol id="svg-doc" viewBox="0 0 24 24">
 <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
   <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>
    <symbol id="svg-search" viewBox="0 0 24 24">
 <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>
<symbol id="svg-copy" viewBox="0 0 16 16">
 <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
   <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
 <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
   <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
   <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>
</svg>
 <div class="side-bar">
 <div class="site-header">
    <a href="/pr-preview/pr-45/" class="site-title lh-tight">
  I2 Intro Neuro/AI
</a>
    <a href="#" id="menu-button" class="site-button">
      <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
    </a>
 </div>
 <nav aria-label="Main" id="site-nav" class="site-nav">
     <ul class="nav-list"><li class="nav-list-item"><a href="/pr-preview/pr-45/announcements/" class="nav-list-link">Announcements</a><li class="nav-list-item"><a href="/pr-preview/pr-45/schedule/" class="nav-list-link">Schedule</a><li class="nav-list-item active"><a href="#" class="nav-list-expander" aria-label="toggle links in Core Content category">
            <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg>
          </a><a href="/pr-preview/pr-45/content/" class="nav-list-link">Core Content</a><ul class="nav-list"><li class="nav-list-item "><a href="/pr-preview/pr-45/content/machine_learning/" class="nav-list-link">Machine Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/deep_learning/" class="nav-list-link">Deep Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/basic_neuro/" class="nav-list-link">Basic Neuroanatomy</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/computer_vision/" class="nav-list-link">Computer Vision</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/visual_system/" class="nav-list-link">Visual System</a><li class="nav-list-item  active"><a href="/pr-preview/pr-45/content/reinforcement_learning/" class="nav-list-link active">Reinforcement Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/ai_ethics/" class="nav-list-link">AI Ethics</a><li class="nav-list-item "><a href="/pr-preview/pr-45/content/language_modeling/" class="nav-list-link">Language Modeling</a></ul><li class="nav-list-item"><a href="#" class="nav-list-expander" aria-label="toggle links in Additional Content category">
            <svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg>
          </a><a href="/pr-preview/pr-45/addcontent/" class="nav-list-link">Additional Content</a><ul class="nav-list"><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/build_a_brain/" class="nav-list-link">Build a Brain</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/cogsci_pain/" class="nav-list-link">Deinforcement Learning</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/fairness_theory/" class="nav-list-link">Fairness & Theory</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/gradient_descent/" class="nav-list-link">Gradient Descent</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/human_char_brain/" class="nav-list-link">Human Brain Characteristics</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/movement/" class="nav-list-link">Movement</a><li class="nav-list-item "><a href="/pr-preview/pr-45/addcontent/creative_neuro/" class="nav-list-link">Neuroscience & Creativity</a></ul><li class="nav-list-item"><a href="/pr-preview/pr-45/staff/" class="nav-list-link">Course Staff</a><li class="nav-list-item"><a href="/pr-preview/pr-45/graduates/" class="nav-list-link">Graduates</a></ul>
 </nav>
   <footer class="site-footer">
      This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.
   </footer>
</div>
 <div class="main" id="top">
   <div id="main-header" class="main-header">
<div class="search">
 <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search I2 Intro Neuro/AI" aria-label="Search I2 Intro Neuro/AI" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
 </div>
 <div id="search-results" class="search-results"></div>
</div>
   <nav aria-label="Auxiliary" class="aux-nav">
 <ul class="aux-nav-list">
     <li class="aux-nav-list-item">
        <a href="https://interactive-intelligence.github.io" class="site-button"
        >
          UW Interactive Intelligence Website
        </a>
 </ul>
</nav>
</div>
   <div id="main-content-wrap" class="main-content-wrap">
   <nav aria-label="Breadcrumb" class="breadcrumb-nav">
     <ol class="breadcrumb-nav-list">
         <li class="breadcrumb-nav-list-item"><a href="/pr-preview/pr-45/content/">Core Content</a>
       <li class="breadcrumb-nav-list-item"><span>Reinforcement Learning</span>
     </ol>
   </nav>
     <div id="main-content" class="main-content" role="main">
         <h1 id="reinforcement-learning">
    <a href="#reinforcement-learning" class="anchor-heading" aria-labelledby="reinforcement-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Reinforcement Learning</u>
</h1>
<p>Welcome to the Reinforcement Learning section of the I2 Course! The topics in this unit should get you really
thinking about NeuroAI. Be sure to ask lots of questions!</p>
<h2 id="technical-track-content">
    <a href="#technical-track-content" class="anchor-heading" aria-labelledby="technical-track-content"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Technical Track Content</u>
</h2>
<h3 id="task-1">
    <a href="#task-1" class="anchor-heading" aria-labelledby="task-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 1:</u></strong>
</h3>
<p><em>Navigate to the relevant section of the I2 Grimoire using the link below. Read the textbook and answer all synthesis questions to the best of your ability. Be sure to save these somewhere for future reference.</em></p>
<h3 id="i2-grimoire-reinforcement-learning">
    <a href="#i2-grimoire-reinforcement-learning" class="anchor-heading" aria-labelledby="i2-grimoire-reinforcement-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a href="https://github.com/interactive-intelligence/I2-grimoire/blob/PDF/units/Reinforcement%20Learning.pdf">I2 Grimoire: Reinforcement Learning</a>
</h3>
   <hr />
<h3 id="task-2">
    <a href="#task-2" class="anchor-heading" aria-labelledby="task-2"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 2:</u></strong>
</h3>
<p><em>Solve the coding challenges within the Jupyter notebook linked below (through Colab). If you encounter any issues with the notebook not functioning as described, please let us know!</em></p>
<p>Please ask questions as you work through this project. Be sure to discuss with others in your group if you have one! Share your answers as you like, the goal is to learn and we’re not holding grades over your head.</p>
<p>In this project, you will be implementing parts of a simple Q-Learning algorithm for a “cartpole” environment.</p>
<p><strong>Colab Link:</strong> <a href="https://colab.research.google.com/github/interactive-intelligence/intro-neuro-ai-website/blob/main/notebooks/unit-06/rl_net.ipynb">Reinforcement Learning Colab Notebook</a> <strong>(1.5 hr)</strong></p>
<p>When you are finished with your code, independently verify that it works and have fun with it! If you add any additional functionality be sure to talk about it with others and give them ideas.</p>
<p>Remember that this is all for your learning, so do your best and don’t stress!</p>
<p>Congratulations! You now understand the (<em>incredibly basic)</em> basics of Deep RL!</p><hr /><hr /><hr />
<h2 id="literacy-track-content">
    <a href="#literacy-track-content" class="anchor-heading" aria-labelledby="literacy-track-content"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <u>Literacy Track Content</u>
</h2>
<h3 id="task-1-1">
    <a href="#task-1-1" class="anchor-heading" aria-labelledby="task-1-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 1:</u></strong>
</h3>
<p><em>Read the article below, and answer any synthesis questions placed along the way.</em></p>
<p>Welcome to one of the most important units in this course—and one of the most challenging! Like all our articles, the goal of this article is to give you an intuitive understanding of reinforcement learning so that you can recognize it in daily life and apply it to technical projects.</p>
<p>Reinforcement learning is the study of how a free object, or <strong>agent</strong>, moves around and accomplishes tasks in an <strong>environment</strong>, either real or simulated. The very general idea involves rewarding the object for desirable actions (i.e. reinforcing that behavior) and punishing it for undesirable actions.</p>
<p>For example, suppose we’re teaching a computer how to play chess against a human. In this case, the agent would be the computer and the environment would be the chess game (i.e. the opponent, board, and pieces).</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/agent_environment.png" alt="Diagram of the agent and environment described above" width="500" />
</div>
<p>The computer starts by taking an <strong>action</strong>–in other words, it does something. In this case, say the computer captures one of the human opponent’s pawns. Now the environment (the chess game) looks different from before the computer took its action; the computer has changed the <strong>state</strong> of the environment.</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/good_action.png" alt="Diagram of the environment after a good action is taken" width="500" />
</div>
<p>The state of the environment is favorable—we’re glad the computer took this action, and we want to reinforce it (we want it to keep taking actions like this). So we give the computer a <strong>reward</strong> that’s proportional to how desirable the action was. In this case, the reward is pretty moderate—capturing a pawn is good, but it’s not one of the best moves the computer can make. If the computer had captured the opponent’s queen, for example, we’d give it more of a reward because that’s a more desirable action.</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/good_reward.png" alt="Diagram of the environment after a reward is given" width="500" />
</div>
<p>Suppose the computer takes a different action: instead of capturing a pawn, it knocks over the entire board. Again, our environment is in a new state as a result of this action.</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/bad_action.png" alt="Diagram of the environment after a bad action is taken" width="500" />
</div>
<p>Unfortunately, this new state is very bad for the computer because there’s no way it can win the game now. So we want to punish this action and make sure the computer avoids it in the future. We can do this by giving it a negative reward to indicate that it’s an undesirable action.</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/bad_reward.png" alt="Diagram of the environment after a punishment is given" width="500" />
</div>
<p>The computer decides what steps to take using something called a <strong>policy</strong>. A computer uses a policy to decide what its next step should be. For example, maybe the computer’s policy is to maximize its rewards. Then, every single action it takes is the action that produces the highest reward. This helps it avoid undesirable actions, like knocking over the board, because they have such low rewards.</p>
<p>This isn’t always the best policy, though. We said earlier that capturing the opponent’s queen is a very high-reward action. Suppose the computer is in a position where it can capture its opponent’s queen, but in doing so leaves its king vulnerable. In this case, maybe taking the highest-reward action isn’t the best way to go. We would have to use a different policy in our decision-making.</p>
<p>Deep reinforcement learning combines deep learning and reinforcement learning. Its goal is to get the computer to learn to do something, and it teaches the computer using the principles of RL.</p>
<p>Take a look at the short video below, which tries to teach a robot to walk using deep reinforcement learning (don’t worry, you’ll have fewer Synthesis Questions to make up for the extra video!).</p>
<h3 id="video-1-ai-learns-to-walk-deep-reinforcement-learning-9-min">
    <a href="#video-1-ai-learns-to-walk-deep-reinforcement-learning-9-min" class="anchor-heading" aria-labelledby="video-1-ai-learns-to-walk-deep-reinforcement-learning-9-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 1:</strong> <a href="https://youtu.be/L_4BPjLBF4E?si=jtH5sPdoihN60MLh">AI Learns to Walk (deep reinforcement learning)</a> (9 min)
</h3>
<div class="center">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/L_4BPjLBF4E?si=OKOq_guDYy8hBWnz" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>
<p>Notice that the robot wasn’t given any directions. It was given a target, and every action it took was rewarded or punished, but it ultimately had to learn the correct sequence of actions that would allow it to walk.</p>
<p>If we apply this to our chess example, the target might be to win the game by capturing the opponent’s king. We won’t tell the computer how to do that, but every time it takes an action we can reward or punish it. That way, it starts to learn the correct sequence of actions that it needs to take in order to win the game.</p>
<p>Also, like in the video, we can put our chess computer in different environments to force it to learn new actions. For example, we can start it out in an environment where its opponent is a three-year-old. As the computer gets better, we can put it in new environments with more and more advanced opponents to force it to learn new skills, in the same way the robot in the video became better at walking by crossing more and more difficult terrain.</p>
<div style="text-align:center">
    <img src="../assets/unit6/literacy_images/opponents.png" alt="Diagram of how the environment gets more advanced" width="500" />
</div>
<p>Now let’s watch some videos on RL.</p>
<h3 id="video-1-reinforcement-learning-crash-course-ai-9-12-min">
    <a href="#video-1-reinforcement-learning-crash-course-ai-9-12-min" class="anchor-heading" aria-labelledby="video-1-reinforcement-learning-crash-course-ai-9-12-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 1:</strong> <a href="https://youtu.be/nIgIv4IfJ6s?si=_ppJ4srBtfy04L7x">Reinforcement Learning: Crash Course AI #9</a> <strong>(12 min)</strong>
</h3>
<div class="center">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/nIgIv4IfJ6s?si=wrB1HOnov8x10WJ0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>
<h3 id="synthesis-questions">
    <a href="#synthesis-questions" class="anchor-heading" aria-labelledby="synthesis-questions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><code class="language-plaintext highlighter-rouge">When does it make sense to use reinforcement learning vs. other methods of machine learning to accomplish a task?</code>
 <li><code class="language-plaintext highlighter-rouge">How do the agent, action, and environment interact in reinforcement learning?</code>
 <li><code class="language-plaintext highlighter-rouge">Give an example of two different policies in a reinforcement learning environment that’s NOT the cookie-jar example from the video (but you can use the chess game, the walking robot, or something you come up with yourself!).</code>
</ul>
<h3 id="video-2-reinforcement-learning-from-scratch-8-min">
    <a href="#video-2-reinforcement-learning-from-scratch-8-min" class="anchor-heading" aria-labelledby="video-2-reinforcement-learning-from-scratch-8-min"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Video 2:</strong> <a href="https://www.youtube.com/watch?v=vXtfdGphr3c">Reinforcement Learning from scratch</a> <strong>(8 min)</strong>
</h3>
<div class="center">
    <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/vXtfdGphr3c?si=gcG5swQQ7qncnt7j" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</div>
<h3 id="synthesis-questions-1">
    <a href="#synthesis-questions-1" class="anchor-heading" aria-labelledby="synthesis-questions-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <em>Synthesis Questions</em>
</h3>
<ul>
 <li><code class="language-plaintext highlighter-rouge">What is the purpose of a sigmoid function, and what does its value tell us? What about an error function?</code>
 <li><code class="language-plaintext highlighter-rouge">Describe the idea of gradient descent and how we use it in reinforcement learning.</code>
</ul><hr />
<h3 id="task-2-1">
    <a href="#task-2-1" class="anchor-heading" aria-labelledby="task-2-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong><u>Task 2:</u></strong>
</h3>
<p><em>Complete the following writing activity.</em></p>
<p>The non-technical project for this unit will involve some writing! <strong>Choose 3</strong> of the prompts below and write <strong>at least 200</strong> (<em>meaningful!</em>) words on each one! We will not be strictly grading you on correctness or anything like that. This is an opportunity to deeply engage with the material you have just learned about, and creatively connect it to neuroscience!</p>
<ul>
 <li>Can you provide examples of experimental evidence linking reinforcement learning algorithms to observed synaptic changes in the brain?
 <li>How do human neural systems encode reward signals and how does this relate to the concept of rewards in reinforcement learning models?
 <li>What ethical considerations should be taken into account when developing interventions based on neuroscientific findings, and how can accountability be established for the potential impacts of such interventions?
 <li>Reflecting on you have learned from this unit, what is one thing you found to be most interesting?
 <li>What is one concept from this unit that you would like to learn more about and why?
</ul>
     </div>
   </div>
<div class="search-overlay"></div>
 </div>
  
